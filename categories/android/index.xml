<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>android on Lei.Chat()</title>
    <link>https://www.lei.chat/categories/android/</link>
    <description>Recent content in android on Lei.Chat()</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>&amp;copy; 2018 - 2026 &lt;a href=&#34;https://www.lei.chat/&#34;&gt;Lei Zhang&lt;/a&gt;
</copyright>
    <lastBuildDate>Sun, 19 Sep 2021 19:17:07 -0400</lastBuildDate><atom:link href="https://www.lei.chat/categories/android/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CodeGen Performant Convolution Kernels for Mobile GPUs</title>
      <link>https://www.lei.chat/posts/codegen-performant-convolution-kernels-for-mobile-gpus/</link>
      <pubDate>Sun, 19 Sep 2021 19:17:07 -0400</pubDate>
      
      <guid>https://www.lei.chat/posts/codegen-performant-convolution-kernels-for-mobile-gpus/</guid>
      <description>&lt;p&gt;This blog post talks about how to generate performant code for convolution ops
using MLIR’s multiple levels of abstractions and transformations.
I initially created it for targeting ARM Mali GPUs in IREE. But given it is
just direct tiling and vectorization, it should be widely applicable.&lt;/p&gt;
&lt;p&gt;I will walk through the lowering steps, so if you are interested to know how to
organize MLIR’s various dialects/patterns together to achieve similar tasks,
this blog post might also be useful.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Android Native Library Benchmarking Pipeline for Open Source Projects</title>
      <link>https://www.lei.chat/posts/android-native-library-benchmarking-pipeline-for-open-source-projects/</link>
      <pubDate>Sat, 21 Aug 2021 22:47:43 -0400</pubDate>
      
      <guid>https://www.lei.chat/posts/android-native-library-benchmarking-pipeline-for-open-source-projects/</guid>
      <description>&lt;p&gt;Today I would like to describe one way to build a scalable and frictionless
benchmarking pipeline for Android native libraries, aiming to support different
benchmark and device variants.
It is for open source projects, so it composes public services, commonly
free under such conditions.
The ingredients are cloud virtual machines for building, local single board
computers (e.g., Raspberry Pi) for hosting Android devices and executing
benchmarks, a &lt;a href=&#34;https://github.com/google/dana&#34;&gt;Dana&lt;/a&gt; server for keeping track of benchmark results of
landed changes, and Python scripts for posting benchmark comparisons to pull
requests.
A &lt;a href=&#34;https://buildkite.com&#34;&gt;Buildkite&lt;/a&gt; pipeline chains them together and drives the full flow.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GPGPU, ML Inference, and Vulkan Compute</title>
      <link>https://www.lei.chat/posts/gpgpu-ml-inference-and-vulkan-compute/</link>
      <pubDate>Sun, 25 Jul 2021 11:25:26 -0400</pubDate>
      
      <guid>https://www.lei.chat/posts/gpgpu-ml-inference-and-vulkan-compute/</guid>
      <description>&lt;p&gt;Nowadays GPUs are utilized for both graphics rendering and general-purpose
compute (GPGPU). For the latter, CUDA is the indisputable leading solution.
Though, with so many other GPU vendors, the quest for a GPGPU standard never
stops. OpenCL was a great attempt and is used widely; but still it falls
short on many aspects.
Given the success of Vulkan in graphics and it being both a graphics and
compute API, one would wonder whether it can actually be the next-generation
GPGPU standard. I certainly believe so; but the road is not full of roses.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Edge/Mobile ML Inference Challenges</title>
      <link>https://www.lei.chat/posts/edge-mobile-ml-inference-challenges/</link>
      <pubDate>Sat, 17 Jul 2021 13:48:27 -0400</pubDate>
      
      <guid>https://www.lei.chat/posts/edge-mobile-ml-inference-challenges/</guid>
      <description>&lt;p&gt;These days if you would like to learn about machine learning, there are
abundant great resources on the web discussing model architectures and how to
code and train them.
Materials about inference, though, are generally much harder to find,
especially for edge and mobile. You might ask, inference is just the forward
pass of training, so how hard can it be? Actually, it faces lots of unique
challenges, to the extent that we are basically solving completely different
major problems.
I have been working on inference at the edge for a while, so let me capture
them in this blog post, by contrasting training and inference in the cloud.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Sampling Performance Counters from Mobile GPU Drivers</title>
      <link>https://www.lei.chat/posts/sampling-performance-counters-from-gpu-drivers/</link>
      <pubDate>Thu, 08 Jul 2021 19:16:41 -0400</pubDate>
      
      <guid>https://www.lei.chat/posts/sampling-performance-counters-from-gpu-drivers/</guid>
      <description>&lt;p&gt;In a &lt;a href=&#34;../android-linux-gpu-drivers-internals-and-resources&#34;&gt;previous blog post&lt;/a&gt; I gave a general introduction
to GPU driver internals in Android/Linux systems. Following up with it, today
I will explain how a specific functionality, hardware performance counter
(perf counter) queries, is handled in both Qualcomm Adreno and ARM Mali drivers,
by walking through the kernel driver source code.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Android/Linux GPU Drivers: Internals and Resources</title>
      <link>https://www.lei.chat/posts/android-linux-gpu-drivers-internals-and-resources/</link>
      <pubDate>Mon, 05 Jul 2021 18:20:07 -0400</pubDate>
      
      <guid>https://www.lei.chat/posts/android-linux-gpu-drivers-internals-and-resources/</guid>
      <description>&lt;p&gt;Recently I have been working on a library that needs to directly interact with
GPU kernel drivers from various vendors on Android/Linux systems. Compared to
various GPU APIs, information at this level is quite sparse; so it is not a
straightforward task, to say the least, and ends up requiring me to piece
multiple sources together to figure out the details. So I am logging these driver
internals and resources down in case it can be useful to others that are
interested in these low-level bits.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
