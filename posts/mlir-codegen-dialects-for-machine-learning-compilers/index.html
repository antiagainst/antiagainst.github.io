<!DOCTYPE html>
<html lang='en' ><meta charset="utf-8">
<meta name="viewport" content="width=device-width">


<title>MLIR CodeGen Dialects for Machine Learning Compilers | Lei.Chat()</title>

<meta name="generator" content="Hugo Eureka 0.8.4" />
<link rel="stylesheet" href="https://www.lei.chat/css/eureka.min.css">
<script defer src="https://www.lei.chat/js/eureka.min.js"></script>

<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload"
  href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,600;0,700;1,400;1,600;1,700&family=Noto+Serif+SC:wght@400;600;700&display=swap"
  as="style" onload="this.onload=null;this.rel='stylesheet'">
<link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro&display=swap" rel="stylesheet">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/styles/solarized-light.min.css"
   media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/highlight.min.js"
   crossorigin></script>

  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/languages/bash.min.js"
     crossorigin></script>

  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/languages/c.min.js"
     crossorigin></script>

  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/languages/cmake.min.js"
     crossorigin></script>

  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/languages/cpp.min.js"
     crossorigin></script>

  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/languages/glsl.min.js"
     crossorigin></script>

  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/languages/javascript.min.js"
     crossorigin></script>

  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/languages/llvm.min.js"
     crossorigin></script>

  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/languages/python.min.js"
     crossorigin></script>

  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/languages/shell.min.js"
     crossorigin></script>

<script defer src="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/js/all.min.js"
   integrity="sha256-uNYoXefWRqv&#43;PsIF/OflNmwtKM4lStn9yrz2gVl6ymo="  crossorigin></script>




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
   integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3&#43;Aro6EYUG4&#43;cU&#43;KJWu/X"  media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" 
  integrity="sha384-g7c&#43;Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI&#43;sEnkvrMWph2EDg4"  crossorigin></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
   integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC&#43;Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa"  crossorigin></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false },
        { left: "\\(", right: "\\)", display: false },
        { left: "\\[", right: "\\]", display: true }
      ],
    });
  });
</script>


<script defer src="https://cdn.jsdelivr.net/npm/mermaid@8.9.2/dist/mermaid.min.js" 
  integrity="sha256-Zmpaaj&#43;GXFsPF5WdPArSrnW3b30dovldeKsW00xBVwE="  crossorigin></script>
<link rel="preconnect" href="https://www.google-analytics.com" crossorigin>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109525036-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());
  gtag('config', 'UA-109525036-1');
</script>


<link rel="icon" type="image/png" sizes="32x32" href="https://www.lei.chat/images/avatar_hudaf23b5d8d39c4f2b01519ceec7d6b7b_105358_32x32_fill_box_center_3.png">
<link rel="apple-touch-icon" sizes="180x180" href="https://www.lei.chat/images/avatar_hudaf23b5d8d39c4f2b01519ceec7d6b7b_105358_180x180_fill_box_center_3.png">

<meta name="description"
  content="The initial blog post in this series captured my overall take
on the evolution trends of compilers and IRs.
It also touched on LLVM IR, SPIR-V, and
MLIR, explaining the problems they are addressing and design
focuses thereof.
Today I will expand on MLIR and talk about its dialect hierarchy for machine
learning (ML) compilers systematically.">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
      "@type": "ListItem",
      "position": 1 ,
      "name":"Posts",
      "item":"https://www.lei.chat/posts/"},{
      "@type": "ListItem",
      "position": 2 ,
      "name":"MLIR CodeGen Dialects for Machine Learning Compilers",
      "item":"https://www.lei.chat/posts/mlir-codegen-dialects-for-machine-learning-compilers/"}]
}
</script>



<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://www.lei.chat/posts/mlir-codegen-dialects-for-machine-learning-compilers/"
    },
    "headline": "MLIR CodeGen Dialects for Machine Learning Compilers | Lei.Chat()",
    "image": "https://www.lei.chat/posts/mlir-codegen-dialects-for-machine-learning-compilers/codegen-dialect-hierarchy.svg",
    "datePublished": "2022-02-20T15:21:03-05:00",
    "dateModified": "2022-02-20T15:21:03-05:00",
    "wordCount":  3334 ,
    "publisher": {
        "@type": "Person",
        "name": "Lei Zhang",
        "logo": {
            "@type": "ImageObject",
            "url": "https://www.lei.chat/images/avatar.png"
        }
        },
    "description": "\u003cp\u003eThe \u003ca href=\u0022..\/compilers-and-irs-llvm-ir-spirv-and-mlir\/\u0022\u003einitial blog post\u003c\/a\u003e in this series captured my overall take\non the evolution trends of compilers and IRs.\nIt also touched on \u003ca href=\u0022..\/compilers-and-irs-llvm-ir-spirv-and-mlir\/#llvm-ir\u0022\u003eLLVM IR\u003c\/a\u003e, \u003ca href=\u0022..\/compilers-and-irs-llvm-ir-spirv-and-mlir\/#spir-v\u0022\u003eSPIR-V\u003c\/a\u003e, and\n\u003ca href=\u0022..\/compilers-and-irs-llvm-ir-spirv-and-mlir\/#mlir\u0022\u003eMLIR\u003c\/a\u003e, explaining the problems they are addressing and design\nfocuses thereof.\nToday I will expand on MLIR and talk about its dialect hierarchy for machine\nlearning (ML) compilers systematically.\u003c\/p\u003e"
}
</script><meta property="og:title" content="MLIR CodeGen Dialects for Machine Learning Compilers | Lei.Chat()" />
<meta property="og:type" content="article" />


<meta property="og:image" content="https://www.lei.chat/images/avatar.png">


<meta property="og:url" content="https://www.lei.chat/posts/mlir-codegen-dialects-for-machine-learning-compilers/" />




<meta property="og:description" content="The initial blog post in this series captured my overall take
on the evolution trends of compilers and IRs.
It also touched on LLVM IR, SPIR-V, and
MLIR, explaining the problems they are addressing and design
focuses thereof.
Today I will expand on MLIR and talk about its dialect hierarchy for machine
learning (ML) compilers systematically." />




<meta property="og:locale" content="en" />



<meta property="og:locale:alternate" content="zh" />




<meta property="og:site_name" content="Lei.Chat()" />






<meta property="article:published_time" content="2022-02-20T15:21:03-05:00" />


<meta property="article:modified_time" content="2022-02-20T15:21:03-05:00" />



<meta property="article:section" content="posts" />


<meta property="article:tag" content="mlir" />

<meta property="article:tag" content="codegen" />

<meta property="article:tag" content="dialect" />

<meta property="article:tag" content="ml" />

<meta property="article:tag" content="compiler" />

<meta property="article:tag" content="tf" />









<meta property="og:see_also" content="https://www.lei.chat/posts/single-node-ml-runtime-foundation/" />



<meta property="og:see_also" content="https://www.lei.chat/posts/mlir-linalg-dialect-and-patterns/" />



<meta property="og:see_also" content="https://www.lei.chat/posts/mlir-vector-dialect-and-patterns/" />





<meta property="og:see_also" content="https://www.lei.chat/posts/compilers-and-irs-llvm-ir-spirv-and-mlir/" />






<body class="flex flex-col min-h-screen">
  <header class="fixed flex items-center w-full min-h-16 pl-scrollbar z-50 bg-secondary-bg shadow-sm">
    <div class="w-full max-w-screen-xl mx-auto"><script>
    let storageColorScheme = localStorage.getItem("lightDarkMode")
    if (((storageColorScheme == 'Auto' || storageColorScheme == null) && window.matchMedia("(prefers-color-scheme: dark)").matches) || storageColorScheme == "Dark") {
        document.getElementsByTagName('html')[0].classList.add('dark')
    }
</script>
<nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0">
    <a href="/" class="mr-6 text-primary-text text-xl font-bold">Lei.Chat()</a>
    <button id="navbar-btn" class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
        <i class="fas fa-bars"></i>
    </button>

    <div id="target"
        class="hidden block md:flex md:flex-grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20">
        <div class="md:flex md:h-16 text-sm md:flex-grow pb-4 md:pb-0 border-b md:border-b-0">
            <a href="/posts/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  selected-menu-item  mr-4">Posts</a>
            <a href="/tags/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  mr-4">Tags</a>
            <a href="/categories/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  mr-4">Categories</a>
            <a href="/series/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  mr-4">Series</a>
            <a href="/authors/me" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  mr-4">About</a>
        </div>

        <div class="flex">
            <div class="relative pt-4 md:pt-0">
                <div class="cursor-pointer hover:text-eureka" id="lightDarkMode">
                    <i class="fas fa-adjust"></i>
                </div>
                <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id="is-open">
                </div>
                <div class="absolute flex flex-col left-0 md:left-auto right-auto md:right-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40"
                    id='lightDarkOptions'>
                    <span class="px-4 py-1 hover:text-eureka" name="Light">Light</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Dark">Dark</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Auto">Auto</span>
                </div>
            </div>
            <div class="relative pt-4 pl-4 md:pt-0">
                <div class="cursor-pointer hover:text-eureka" id="languageMode">
                    <i class="fas fa-globe"></i>
                    <span class="pl-1">English</span>
                </div>
                <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id="is-open-lang">
                </div>
                <div class="absolute flex flex-col left-0 md:left-auto right-auto md:right-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40"
                    id='languageOptions'>
                    <a class="px-4 py-1 hover:text-eureka" href="https://www.lei.chat/posts/mlir-codegen-dialects-for-machine-learning-compilers/">English</a>
                    <a class="px-4 py-1 hover:text-eureka" href="https://www.lei.chat/zh/posts/mlir-codegen-dialects-for-machine-learning-compilers/">简体中文</a>
                </div>
            </div>
        </div>
    </div>

    <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id="is-open-mobile">
    </div>

</nav>
<script>
    
    let element = document.getElementById('lightDarkMode')
    if (storageColorScheme == null || storageColorScheme == 'Auto') {
        document.addEventListener('DOMContentLoaded', () => {
            window.matchMedia("(prefers-color-scheme: dark)").addEventListener('change', switchDarkMode)
        })
    } else if (storageColorScheme == "Light") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'sun')
        element.firstElementChild.classList.add('fa-sun')
    } else if (storageColorScheme == "Dark") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'moon')
        element.firstElementChild.classList.add('fa-moon')
    }

    document.addEventListener('DOMContentLoaded', () => {
        getcolorscheme();
        switchBurger();
        switchLanguage()
    });
</script>
</div>
  </header>
  <main class="flex-grow pt-16">
    <div class="pl-scrollbar">
      <div class="w-full max-w-screen-xl lg:px-4 xl:px-8 mx-auto">


<div class="grid grid-cols-2 lg:grid-cols-8 gap-4 lg:pt-12">
    <div
        class="col-span-2  lg:col-span-6 bg-secondary-bg rounded px-6 py-8">
        <h1 class="font-bold text-3xl text-primary-text">MLIR CodeGen Dialects for Machine Learning Compilers</h1>
        <div class="flex flex-wrap flex-row items-center mt-2 text-tertiary-text">
    <div class="mr-6 my-2">
        <i class="fas fa-calendar mr-1"></i>
        <span>2022-02-20</span>
    </div>
    <div class="mr-6 my-2">
        <i class="fas fa-clock mr-1"></i>
        <span>16 min read</span>
    </div>
    
    
    
    <div class="mr-6 my-2">
        <i class="fas fa-folder mr-1"></i>
        
        <a href="https://www.lei.chat/categories/compiler/" class="hover:text-eureka">compiler</a><span>, </span>
        
        
        <a href="https://www.lei.chat/categories/ir/" class="hover:text-eureka">ir</a><span>, </span>
        
        
        <a href="https://www.lei.chat/categories/mlir/" class="hover:text-eureka">mlir</a><span>, </span>
        
        
        <a href="https://www.lei.chat/categories/ml-inference/" class="hover:text-eureka">ml-inference</a>
        
    </div>
    

    
    <div class="mr-6 my-2">
        <i class="fas fa-th-list mr-1"></i>
        
        <a href="https://www.lei.chat/series/compiler-development/" class="hover:text-eureka">compiler-development</a>
        
    </div>
    
</div>

        
        <div class="content">
            <p>The <a href="../compilers-and-irs-llvm-ir-spirv-and-mlir/">initial blog post</a> in this series captured my overall take
on the evolution trends of compilers and IRs.
It also touched on <a href="../compilers-and-irs-llvm-ir-spirv-and-mlir/#llvm-ir">LLVM IR</a>, <a href="../compilers-and-irs-llvm-ir-spirv-and-mlir/#spir-v">SPIR-V</a>, and
<a href="../compilers-and-irs-llvm-ir-spirv-and-mlir/#mlir">MLIR</a>, explaining the problems they are addressing and design
focuses thereof.
Today I will expand on MLIR and talk about its dialect hierarchy for machine
learning (ML) compilers systematically.</p>
<p>To remind, MLIR is a general compiler infrastructure; it is not specifically for
ML and can support building any domain specific compilers.
But its application to the ML domain is indeed where we see the most vigorous
development, especially for writing compilers to convert models from existing
ML frameworks and target heterogeneous hardware.</p>
<h2 id="building-blocks">Building Blocks</h2>
<p>One great strength of a compiler-based approach is its <strong>composability</strong>.
For example, if individual features A, B, and C work, their various combinations
should generally work.
This nature is one of compilers' key differences from libraries, where we can
see different combinations go to completely separate manually-tuned paths.
Turning a combinatorial problem into a linear one is a great save on
engineering efforts.</p>
<p>Factoring out basic building blocks appropriately is important for achieving
such composability.
In IRs, it&rsquo;s common to define suitable operations for such purposes.
But for ML, it&rsquo;s hard to have a properly layered and well organized stack by
only using operations, because of the huge semantic gap between source programs
and target hardware, and the breadth of sources and targets.
For this, MLIR enables a higher level of building blocks using dialects.</p>
<p>A dialect is basically a namespace that contains a coherent set of operations
and associated supporting types and attributes to model some concept or
abstraction.
A ML compiler pieces together dialects, with its own customization and
extensions when necessary. MLIR dialects have a few interesting characteristics
worth mentioning&mdash;</p>
<!--
It also brings about progressive lowering across system boundaries to channel
different ML software stacks.
I won't belabor the value propositions of MLIR again here; you can refer to
the [earlier blog post][blog-mlir].

To fulfill all these purposes, dialects are essential building blocks.
-->
<h3 id="operations-carrying-structures">Operations carrying structures</h3>
<p>Operations are the atomic entities in compilers, for both representation and
transformation.
We have flexibility to put operations in basic blocks and then in functions.
But those are just two thin levels of structures; semantics still largely
depends on individual operations and pattern matching happens on (meshes of)
individual operations.
It&rsquo;s rather difficult to &ldquo;customize&rdquo; existing operations or compose operations
in a tight fashion to set &ldquo;boundaries&rdquo; for applying patterns.</p>
<p>One key feature of MLIR operations is it can nest structures internally,
using the <a href="https://mlir.llvm.org/docs/LangRef/#regions">region</a> mechanism.
This enables a plethora of <em>structured</em> operations carrying <em>payloads</em>.
Those operations themselves define semantics for some structure (e.g., control
flow) that can be attached to the payload operations;
the payload-carrying operations and payload operations compose and extend
each other.
A prominent example is <a href="https://mlir.llvm.org/docs/Dialects/Linalg/#payload-carrying-opsa-namepayload_opsa">the <code>linalg.generic</code> op</a>;
functions and modules under the hood are also such payload-carrying ops
in MLIR.
Regions keep those payload ops contained as pattern application boundaries
in general.</p>
<!--
Payload-carrying structured ops are great building blocks that allows
customization.
-->
<h3 id="types-signaling-abstraction-levels">Types signaling abstraction levels</h3>
<p>Ultimately, operations are just some sort of &ldquo;computation&rdquo; on values of certain
types.
It&rsquo;s types that represent the abstraction levels they operate on more
intimately.
For example, we can have an addition operation on either tensors, buffers, or
scalars.
There are not many differences among the addition operations per se, but they
clearly are at different levels&mdash;tensors belong to high-level ML frameworks or
programming models, buffers map to middle-level memory hierarchies on execution
machines, and scalars are for low-level registers inside the chip.</p>
<p>A MLIR dialect has the freedom to define its own types.
The core infrastructure tries its best to treat types from various dialects
equally and provide generic mechanisms like <a href="https://mlir.llvm.org/docs/DialectConversion/#type-conversion"><em>type conversion</em></a>
to facilitate handling types.
Dialect A can also reuse types form dialect B directly or compose them further,
e.g., put primitive types in container types.
A dialect can also define rules to convert from/to types defined in other
dialects, inject those rules into a type converter, and let all the rules
compose; so that the framework can figure out a path to convert types.
But in general, type composition and conversion is trickier and subject to
more restrictions than operations.</p>
<h3 id="dialects-as-modeling-granularity">Dialects as modeling granularity</h3>
<p>Allowing defining types and operations processing those types, dialects are the
granularity for conceptual modeling.
If two dialects operate on the same set of types, they are largely at the
same abstraction level.
It follows that converting dialects with different types are converting
different abstraction levels.</p>
<p>To make life easier, we would typically lower from high- to low-level via some
sort of decomposition (tiling, vectorization, etc.) and lowering from abstract
to concrete concepts via some sort of resource assignment (bufferization,
register allocation, etc.).
But these steps are still hard due to the fact that different abstractions
offer different correctness and performance characteristics.
So it&rsquo;s not surprising that <a href="https://mlir.llvm.org/docs/DialectConversion/"><em>dialect conversion</em></a> arguably
embodies the most complexity among various MLIR mechanisms.</p>
<h2 id="dialect-hierarchy">Dialect Hierarchy</h2>
<p>The above section talked about dialects as the building blocks for piecing
together ML compilers abstractly.
Both types and operations are composable and extensible to make it happen.
In this section I&rsquo;ll be more concrete by talking about existing dialects
and putting them into a hierarchy to show the flow.
The goal here is to focus on major components so that it&rsquo;s easier to see the
overall picture. So this is not meant to be a complete survey of existing
dialects.</p>
<h3 id="problem-space">Problem space</h3>
<p>First let&rsquo;s revisit the problem space and define the scope of the discussion.
ML compilers faces both <strong>depth</strong> and <strong>breadth</strong> challenges&mdash;</p>
<ul>
<li>At the top level, ML models are typically authored with some framework using
Python.
The source program, or programming model, contains high-level operations
operating on high-D tensors.
At the bottom level, the main computation of the model is typically executed
by some dedicated vector/SIMD instruction or special accelerator.
The target hardware, or machine model, only provides low-level instructions
operating on low-D vectors or scalars.</li>
<li>There are a plethora of ML frameworks one can use to write ML models.
There is also a lot of hardware that can execute ML models, which is just
computation at its core.
Hardware targets can offer different compute and memory hierarchies, but it&rsquo;s
common to see tiled-based architectures in CPU, GPU, and various accelerators.
Other than CPU, these hardware targets typically cannot execute the full
ML model, which requires supporting arbitrary control flow and various
synchronization mechanisms. So the CPU is still needed for orchestration.</li>
</ul>
<p>A real end-to-end ML compiler targeting heterogeneous hardware needs to take in
a source ML model and generate both kernels running on accelerators and
scheduling and synchronization logic running on CPU.
There are existing dialects for all of them.
In this blog post, the focus is on those for kernel CodeGen side; I&rsquo;ll leave
scheduling and synchronization (e.g., <a href="https://mlir.llvm.org/docs/Dialects/AsyncDialect/">the <code>async</code> dialect</a> in
MLIR, <a href="https://github.com/google/iree/tree/5947c0837f8bba664babd4b4383a955a7249be64/iree/compiler/Dialect/Stream">the <code>stream</code> dialect</a> in IREE), which traditionally
belong to the runtime, to a later blog post.</p>
<h3 id="overall-picture">Overall picture</h3>
<p>From the type&rsquo;s perspective, a layered stack needs to have proper modeling for
tensors, buffers, vectors, and scalars, and provide support to decompose and
lower them gradually.
From the operation&rsquo;s perspective, we need to have computation and control flow.
Control flow can be explicit as branches, or implicit as the innate structure
of payload-carrying operations. Using these as the dimensions to put various
dialects in a lowering hierarchy:</p>
<p><img src="codegen-dialect-hierarchy.svg" alt="MLIR CodeGen Dialect Hierarchy" title="MLIR CodeGen Dialect Hierarchy"></p>
<h3 id="high-level-model-dialects">High-level model dialects</h3>
<p>From the top we have the source model expressed via some ML framework.
The source model is typically immediately imported into MLIR system with a
framework-specific dialect, e.g., <a href="https://github.com/tensorflow/tensorflow/tree/1cffc0a1946413f8d0237e67d29891649f789c87/tensorflow/compiler/mlir/tensorflow/ir">the <code>tf</code> dialect</a> for TensorFlow,
<a href="https://github.com/tensorflow/tensorflow/tree/1cffc0a1946413f8d0237e67d29891649f789c87/tensorflow/compiler/mlir/lite/ir">the <code>tfl</code> dialect</a> for TFLite, and <a href="https://github.com/llvm/torch-mlir/tree/abbde7d439836ab359f64f84ba1f1740f06c765b/include/torch-mlir/Dialect/Torch/IR">the <code>torch</code>
dialect</a> for PyTorch).
The purpose of these dialects is to faithfully represent the source model for
the specific framework.
So they typically directly reside in the framework repo given the tight
relationship.</p>
<p>The depth and breadth challenges would require a ML compiler stack to have an
hourglass structure to be manageable.
The next step is to consolidate various framework representations into some
common ML model dialect that serve as inputs to the further shared lowering
steps.
This is a place where we expect to see further development, in the hope that
eventually we will have a coherent set of definitions (either in one or more
dialects) to fully support representing various ML models from different
frameworks and provide the necessary compatibility guarantees, etc.
But as of today, we have <a href="https://github.com/tensorflow/mlir-hlo">the <code>mhlo</code> dialect</a> and <a href="https://mlir.llvm.org/docs/Dialects/TOSA/">the <code>tosa</code>
dialect</a> here.
The former is a descendant of <a href="https://www.tensorflow.org/xla">XLA</a> and is the bridge for TensorFlow;
the latter is a specification with precise numeric guarantees, increasingly
used for multiple frameworks.</p>
<h3 id="middle-level-lowering-dialects">Middle-level lowering dialects</h3>
<p>Now we are at the middle level of the compiler.
High- or low-level dialects are typically at the boundary of MLIR systems and
need to faithfully model some external entities.
Middle-level dialects do not have such constraints, so they can enjoy much more
design flexibility.</p>
<p>Middle-level dialects can be viewed as <em>partial</em> IRs, compared to traditional
IRs like LLVM IR or SPIR-V, which are <em>complete</em> in the sense that they are
self-contained and have all necessary instructions to represent whole CPU/GPU
programs.
This is meant for decoupling and composability.
We mix multiple dialects at this level to represent the original model,
some for the computation or payload, some for the control flow or structure.</p>
<h4 id="the-linalg-dialect">The linalg dialect</h4>
<p><a href="https://mlir.llvm.org/docs/Dialects/Linalg/">The <code>linalg</code> dialect</a> is one of the major dialects for
representing structures.
At its core, a <code>linalg</code> op captures a perfect loop nest&mdash;its indexing maps
specify how the loop induction variables access its operands or results.
The region inside a <code>linalg</code> op captures the payload computation happening
inside the loop nest.
The perfect loop nest is implicit, i.e., there are no explicit loop structures
in the IR.
This key property helps to simplify many analyses and transformations.
For example, to fuse two perfect loop nests, traditionally we need to analyze
the range of each induction variable and how they access elements; it&rsquo;s quite
convoluted.
With indexing maps to implicitly represent the loop nest, it&rsquo;s <a href="https://github.com/llvm/llvm-project/blob/8f310d1967c20d348c617af3a30999031c71fee0/mlir/lib/Dialect/Linalg/Transforms/Fusion.cpp#L537">as simple as
performing <code>inverse(producerIndexMap).compose(consumerIndexMap)</code></a>.
There are also other key design considerations behind <code>linalg</code> ops <a href="https://mlir.llvm.org/docs/Dialects/Linalg/#high-level-description-of-linalg-opsa-namelinalg_opsa">described
in the documentation</a>, which is well worth a
read.</p>
<p>There are many ops in the <code>linalg</code> dialect; two large categories are structured
&ldquo;generic&rdquo; ops and &ldquo;named&rdquo; ops.
The former contains only <a href="https://mlir.llvm.org/docs/Dialects/Linalg/#linalggeneric-mlirlinalggenericop">the <code>linalg.generic</code> op</a>, which is
really the core and &ldquo;raw&rdquo; form of structured <code>linalg</code> ops.
Named <code>linalg</code> ops like <a href="https://github.com/llvm/llvm-project/blob/8f310d1967c20d348c617af3a30999031c71fee0/mlir/python/mlir/dialects/linalg/opdsl/ops/core_named_ops.py#L10-L21">the <code>linalg.matmul</code> op</a> and <a href="https://github.com/llvm/llvm-project/blob/8f310d1967c20d348c617af3a30999031c71fee0/mlir/python/mlir/dialects/linalg/opdsl/ops/core_named_ops.py#L222-L336">various
<code>linalg.conv*</code> ops</a> are just sugar over the <code>linalg.generic</code> op,
with known specific indexing maps and payload computations.
One can <a href="https://github.com/llvm/llvm-project/blob/8f310d1967c20d348c617af3a30999031c71fee0/mlir/lib/Dialect/Linalg/Transforms/Generalization.cpp">convert</a> from the named form to generic forms
easily.
Having a consistent structure among various <code>linalg</code> ops simplifies
transformations because transformations can just be written against indexing
maps and regions regardless of the specific op.</p>
<p>The <code>linalg</code> dialect can operate on both tensors and buffers.
In MLIR, they are represented by the <code>tensor</code> type and <code>memref</code> type,
respectively.
Both are high-level N-D abstractions and can be of dynamic shapes.</p>
<h4 id="tensors-tiling-and-fusion">Tensors, tiling and fusion</h4>
<p>Both the <code>mhlo</code> and <code>tosa</code> dialects convert to the <code>linalg</code> dialect.
The conversion will remain in tensor abstractions, so based on previous
discussions, this is not really converting different abstraction levels.
It&rsquo;s converting op representations for further transformations&mdash;although
we have like the <a href="https://www.tensorflow.org/mlir/hlo_ops#mhlodot_general_mlirmhlodotgeneralop"><code>mhlo.dot_general</code> op</a> or <a href="https://mlir.llvm.org/docs/Dialects/TOSA/#tosamatmul-mlirtosamatmulop"><code>tosa.matmul</code>
op</a> representing batch matmul and arguably they are not that
different from <a href="https://mlir.llvm.org/docs/Dialects/Linalg/#linalgbatch_matmul-mlirlinalgbatchmatmulop">the <code>linalg.batch_matmul</code> op</a>,
the <code>linalg.batch_matmul</code> op has the implicit loop nest making it great for
transformations like <a href="https://github.com/llvm/llvm-project/blob/14f143c9084fc49b45f30a199dc8a16b7506f959/mlir/lib/Dialect/Linalg/Transforms/FusionOnTensors.cpp#L412"><em>tiling</em> and <em>fusion</em></a>, which are crucial
for generating code towards tiled-based architectures.
We just need to materialize some loop nests and shrink the scale of <code>linalg</code>
ops down to slices.</p>
<p>As an example, say we have the following input ML ops:</p>
<pre><code>%0 = &quot;tosa.conv2d&quot;(%input, %filter, %bias)
       {dilation = [1, 1], pad = [0, 0, 0, 0], stride = [2, 2]}
     : (tensor&lt;1x225x225x3xf32&gt;, tensor&lt;32x3x3x3xf32&gt;, tensor&lt;32xf32&gt;)
     -&gt; tensor&lt;1x112x112x32xf32&gt;
</code></pre>
<p>Converting to <code>linalg</code> ops and performing tiling and fusion gives us:</p>
<pre><code>%0 = scf.for %iv0 = ... to ... step ... iter_args(...) -&gt; (tensor&lt;1x112x112x32xf32&gt;) {
  %1 = scf.for ... {
    %input_slice = tensor.extract_slice ...
    %filter_slice = tensor.extract_slice ...
    %bias_slice = tensor.extract_slice ...
    %conv = linalg.conv_2d_nhwc_hwcf {...} ins(%input_slice, %filter_slice) ...
    %generic = linalg.generic ins(%conv, %bias_slice} ... {
      %add = arith.addf ...
      linalg.yield %add ...
    }
    scf.yield %generic
  }
  scf.yield %1
}
</code></pre>
<h4 id="buffers-distribution">Buffers, distribution</h4>
<p>Thus far we are only working on tensors.
Tensors are immutable values that are integral identity and have no side
effects.
SSA def-use chain can be used for data flow analysis.
This makes transforming tensor ops simple.
But still, at some point we need to assign tensors to buffers.
This is called <a href="https://mlir.llvm.org/docs/Bufferization/"><em>bufferization</em></a> in mlir.
Buffers are mutable and can alias; transformations on buffers may require
convoluted dependency and aliasing analyses.
So in MLIR the general trend is to push bufferization to a later stage when
possible, like, after vectorization, to move other transformations forward and
make it as mechanical.</p>
<p>Bufferization is really a conversion between different abstraction levels.
We go from abstract values into concrete resources residing in memory.
How to map various tensors to buffers is both a technical and policy issue,
as we would like to avoid hazards and unnecessary copies.
This part is still evolving fast in MLIR.
After bufferization, we can perform <em>distribution</em> to distribute different
problem tiles to different hardware tiles (CPU threads, GPU workgroups,
GPU workitems, etc.).
The previous example would become:</p>
<pre><code>scf.for %ivz = (%idz * %tilez) to %ubz step (%countz * %tilez) {
  scf.for ... {
    %input_subview = memref.subview ...
    %filter_subview = memref.subview ...
    %bias_subview = memref.subview ...
    %output_subview = memref.subview ...
    linalg.conv_2d_nhwc_hwcf {...}
      ins(%input_subview, %filter_subview) outs(%output_subview) ...
    linalg.generic
      ins(%output_subview, %bias_subview) outs(%ouput_subview) ... {
      %add = arith.addf ...
      linalg.yield %add ...
    }
  }
}
</code></pre>
<h4 id="the-tensor-memref-arith-math-dialect">The tensor, memref, arith, math dialect</h4>
<p>The flow in the above uses ops from the <a href="https://mlir.llvm.org/docs/Dialects/TensorOps/"><code>tensor</code> dialect</a>,
<a href="https://mlir.llvm.org/docs/Dialects/MemRef/"><code>memref</code> dialect</a>, <a href="https://mlir.llvm.org/docs/Dialects/ArithmeticOps/"><code>arith</code> dialect</a>, and
<a href="https://mlir.llvm.org/docs/Dialects/MathOps/"><code>math</code> dialect</a>.</p>
<p><code>tensor</code> and <code>memref</code> dialects contain ops for handling tensors and buffers
respectively. In the above flow they are used to facilitate representing
the tiled IR structure (with <code>tensor.*slice</code> and <code>memref.subview</code> ops).
They can additionally be used for tensor/memref generation, shape manipulation,
and others that don&rsquo;t fit into the payload plus structure paradigm.</p>
<p><code>arith</code> and <code>math</code> ops are for various computation, with the former for basic
integer and floating point operations, and the later for more advanced ones.
They are just the payload ops to compose with payload carrying structured ops,
and can actually operate on multiple abstraction levels, including tensors,
vectors, and scalars.
So we see they appear basically at all steps in the previous graph.</p>
<h4 id="the-vector-dialect">The vector dialect</h4>
<p>Aside from the <code>linalg</code> dialect, <a href="https://mlir.llvm.org/docs/Dialects/Vector/">the <code>vector</code> dialect</a> is
another major dialect for structured code generation.</p>
<p>If we say that tensors are at the abstract programming model level and buffers
are at the concrete machine memory level, then vectors are at chip register
level.
They are closer to the hardware architecture and thus face more reality
constraints.
We can have an unlimited number of tensors in a model.
Bufferization is one level of resource allocation&mdash;it maps them to buffers in
memory. Along the way, we can reuse the same buffer for less memory footprint
and eliding copies. But in general, memory is flexible (e.g., we can dynamically
index into it) and offer large capacity.
Vectors are quite different&mdash;they require static shapes and there are often
very limited amounts of registers in a chip.
How to best utilize the registers and vector/SIMD/SIMT compute units is another
level of resource allocation that is subject to many trade-offs.</p>
<p>In MLIR, the <code>vector</code> dialect is <a href="https://mlir.llvm.org/docs/Dialects/Vector/#components-of-a-generic-retargetable-vector-level-dialect">multiple-level by itself</a>.
Aside from the machine-native vector operations, it also supports high-dimension
virtual vectors and machine-agnostic operations.
The general idea is to progressive lowering to decompose high-dimension vectors
into low-dimension ones and go from machine-agnostic to machine-native.</p>
<h4 id="vectorization-unrolling-hoisting-canonicalization">Vectorization, unrolling, hoisting, canonicalization</h4>
<p>Using the <code>linalg</code> dialect we can tile operations to create static-sized tiles.
Then <a href="https://github.com/llvm/llvm-project/blob/14f143c9084fc49b45f30a199dc8a16b7506f959/mlir/lib/Dialect/Linalg/Transforms/Vectorization.cpp"><em>vectorization</em></a> can kick in to vectorize tiles from
tensors/buffers to vectors of the same rank and shape.
Vectorization creates <a href="https://mlir.llvm.org/docs/Dialects/Vector/#vectortransfer_read-mlirvectortransferreadop"><code>vector.transfer_read</code> ops</a> to read
data from tensors or buffers into virtual vectors, creates <code>vector</code> (e.g.,
<a href="https://mlir.llvm.org/docs/Dialects/Vector/#vectorextract-mlirvectorextractop">the <code>vector.contract</code> op</a>) or <code>arith</code> ops to compute on them,
and then creates <a href="https://mlir.llvm.org/docs/Dialects/Vector/#vectortransfer_write-mlirvectortransferwriteop"><code>vector.transfer_write</code> ops</a> to write
the result back.
Vector transfer ops are powerful abstractions that can model various modes of
load/store from memory, including stride and padding supports.</p>
<p>Here it is different from traditional vectorization, where we need to raise the
abstraction level by going from scalars to vectors.
In MLIR, vectorization converts abstraction levels, but it is still mostly
mechanical due to the fact that we maintain the original shape.</p>
<p>After vectorization, we can then perform <a href="https://github.com/llvm/llvm-project/blob/14f143c9084fc49b45f30a199dc8a16b7506f959/mlir/include/mlir/Dialect/Vector/Transforms/VectorRewritePatterns.h#L280-L311"><em>unrolling</em></a> and
<em>decomposition</em> to <a href="https://github.com/llvm/llvm-project/blob/14f143c9084fc49b45f30a199dc8a16b7506f959/mlir/include/mlir/Dialect/Vector/Transforms/VectorRewritePatterns.h#L130-L175">lower</a> the high-dimension vector ops into
low-dimension to match the target architecture.
Machine-agnostic <code>vector</code> ops can also be lowered to machine-native ones, for
example, converting <code>vector.contract</code> ops into <a href="https://mlir.llvm.org/docs/Dialects/Vector/#vectorfma-mlirvectorfmaop"><code>vector.fma</code> ops</a>.</p>
<p>After unrolling, decomposition, and lowering, <a href="https://github.com/llvm/llvm-project/blob/14f143c9084fc49b45f30a199dc8a16b7506f959/mlir/include/mlir/Dialect/Linalg/Transforms/Hoisting.h"><em>hoisting</em></a> and and
various other <em>canonicalization</em> help to clean up the IR, especially to cancel
various vector read/write or insert/extract pairs.</p>
<p>Now our example should look like:</p>
<pre><code>scf.for %ivz = (%idz * %tilez) to %ubz step (%countz * %tilez) {
  scf.for ... {
    %input_subview = memref.subview ...
    %filter_subview = memref.subview ...
    %bias_subview = memref.subview ...
    %output_subview = memref.subview ...
    vector.transfer_read %input_subview ...
    vector.transfer_read %filter_subivew ...
    ...
    %v0 = vector.fma ...
    %v1 = vector.fma ...
    ...
    vector.transfer_write %v0, %output_subview ...
    vector.transfer_write %v1, %output_subview ...
    ...
  }
}
</code></pre>
<p>The <code>vector</code> dialect uses intra-dialect conversions for progressive lowering.
The patterns are typically minimal and mechanical, but together they really
compose and show great power.
Though properly ordering and applying them is a bit tricky.</p>
<h4 id="the-scf-cf-dialect">The scf, cf dialect</h4>
<p>After the <code>linalg</code> dialect, <a href="https://mlir.llvm.org/docs/Dialects/SCFDialect/">the <code>scf</code> dialect</a> is used as the
structure to hold payload ops.
The <code>scf</code> dialect contains structured control flow ops, notably <a href="https://mlir.llvm.org/docs/Dialects/SCFDialect/#scfif-mlirscfifop">the <code>scf.if</code>
op</a> for conditions and <a href="https://mlir.llvm.org/docs/Dialects/SCFDialect/#scffor-mlirscfforop">the <code>scf.for</code> op</a> op for loops.
These ops again explicitly capture loop ranges and use regions as the
boundaries, making analysis and transformation easier.
Once we are at the final form of control flow, distributed loop nests can be
elided given they only have one trip now. The rest can be lowered into
traditional <a href="https://mlir.llvm.org/docs/Dialects/ControlFlowDialect/"><code>cf</code> ops</a> with basic blocks.</p>
<p>We are almost at the end of the conversion flow.
Next is to perform full dialect conversion to export into another system.</p>
<h3 id="low-level-target-dialects">Low-level target dialects</h3>
<p>At the low-level, we have two dialects in MLIR right now&mdash;<a href="https://mlir.llvm.org/docs/Dialects/LLVM/">the <code>llvm</code>
dialect</a> and <a href="https://mlir.llvm.org/docs/Dialects/SPIR-V/">the <code>spv</code> dialect</a>.
They model the LLVM IR and SPIR-V respectively.
Converting to them prepares the IR for exporting to external systems.
Given they model external IRs, they are subject to constraints from external
IRs, including ops and types.
And the conversion is full dialect conversion, which eliminates any ops not
in <code>llvm</code> or <code>spv</code> dialect.</p>
<p>Typically no major optimizations are expected to happen in the low-level
dialects; those should be done at higher levels.
Transformations here are mostly generic canonicalization and cleanup, and
some additional passes for legality guarantees.</p>
<h2 id="closing-words">Closing Words</h2>
<p>This blog post turns out to be lengthier than I originally planned again.
Thanks for reading this one through!
A quick recap&mdash;</p>
<p>ML compilers face both depth and breadth challenges.
Dialects are the higher level of building blocks provided in MLIR to address
these challenges.
Ideally ML compilers would just need to piece together dialects, with its own
customization and extensions when necessary.
This is a larger granularity than trying to fit together all operations from
different levels and sources and targets.
Each dialect is a coherent set of operations and types.
So it&rsquo;s much more manageable this way and would result in a better layered and
organized stack.
Though this vision might take quite some time to fully realize!</p>
<p>I put major CodeGen dialects in a conversion flow to show their hierarchy,
and also talked about major transformations along the way.
In general, MLIR favors lowering from high- to low-level via some sort of
decomposition (tiling, vectorization, etc.) and from abstract to concrete
concepts via some sort of resource assignment (bufferization, register
allocation, etc.).
Dialects and patterns are means to achieve that, and they  are designed to
require minimal analyses and compose well.</p>
<p>If you want to learn more details, <a href="https://llvm.discourse.group/t/codegen-dialect-overview/2723">Alex&rsquo;s post in Discourse</a> is a
great read. Also <a href="https://arxiv.org/abs/2202.03293">this new MLIR paper</a> discussing CodeGen
specifically by many of my colleagues.
In a future blog post, I&rsquo;ll talk about the runtime and scheduling side.
Stay tuned!</p>
        </div>
        
        <div class="my-4">
    
    <a href="https://www.lei.chat/tags/mlir/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>mlir</a>
    
    <a href="https://www.lei.chat/tags/codegen/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>codegen</a>
    
    <a href="https://www.lei.chat/tags/dialect/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>dialect</a>
    
    <a href="https://www.lei.chat/tags/ml/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>ml</a>
    
    <a href="https://www.lei.chat/tags/compiler/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>compiler</a>
    
    <a href="https://www.lei.chat/tags/tf/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>tf</a>
    
    <a href="https://www.lei.chat/tags/torch/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>torch</a>
    
    <a href="https://www.lei.chat/tags/mhlo/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>mhlo</a>
    
    <a href="https://www.lei.chat/tags/tosa/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>tosa</a>
    
    <a href="https://www.lei.chat/tags/linalg/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>linalg</a>
    
    <a href="https://www.lei.chat/tags/tensor/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>tensor</a>
    
    <a href="https://www.lei.chat/tags/memref/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>memref</a>
    
    <a href="https://www.lei.chat/tags/vector/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>vector</a>
    
    <a href="https://www.lei.chat/tags/scf/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>scf</a>
    
    <a href="https://www.lei.chat/tags/cf/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>cf</a>
    
    <a href="https://www.lei.chat/tags/arith/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>arith</a>
    
    <a href="https://www.lei.chat/tags/math/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>math</a>
    
    <a href="https://www.lei.chat/tags/llvm/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>llvm</a>
    
    <a href="https://www.lei.chat/tags/spirv/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>spirv</a>
    
    <a href="https://www.lei.chat/tags/hierarchy/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>hierarchy</a>
    
    <a href="https://www.lei.chat/tags/type/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>type</a>
    
    <a href="https://www.lei.chat/tags/operation/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>operation</a>
    
    <a href="https://www.lei.chat/tags/tiling/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>tiling</a>
    
    <a href="https://www.lei.chat/tags/fusion/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>fusion</a>
    
    <a href="https://www.lei.chat/tags/distribution/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>distribution</a>
    
    <a href="https://www.lei.chat/tags/vectorization/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>vectorization</a>
    
    <a href="https://www.lei.chat/tags/unrolling/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>unrolling</a>
    
    <a href="https://www.lei.chat/tags/hoisting/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>hoisting</a>
    
    <a href="https://www.lei.chat/tags/canonicalization/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka"><i class="fas fa-tags mr-1"></i>canonicalization</a>
    
</div>

        
        
        


        
        
        
        
<div class="flex flex-col md:flex-row md:justify-between -mx-2 mt-4 px-2 pt-4 border-t">
    <div>
        
        <span class="block font-bold">Previous</span>
        <a href="https://www.lei.chat/posts/mlir-vector-dialect-and-patterns/" class="block">MLIR Vector Dialect and Patterns</a>
        
    </div>
    <div class="md:text-right mt-4 md:mt-0">
        
        <span class="block font-bold">Next</span>
        <a href="https://www.lei.chat/posts/compilers-and-irs-llvm-ir-spirv-and-mlir/" class="block">Compilers and IRs: LLVM IR, SPIR-V, and MLIR</a>
        
    </div>
</div>

        



  <script id="utterances" src="https://utteranc.es/client.js"
            issue-term=title
            repo=antiagainst/antiagainst.github.io
              theme=preferred-color-scheme
        crossorigin="anonymous"
        async>
</script>
<script>
    if (storageColorScheme == "Light") {
      document.getElementById('utterances').setAttribute('theme', 'github-light')
    } else if (storageColorScheme == "Dark") {
      document.getElementById('utterances').setAttribute('theme', 'github-dark')
    }
</script>

    </div>
    
    <div class="col-span-2">
        
        
<div class="bg-secondary-bg rounded p-6">
    <h3 class="text-lg font-semibold mb-4">Series of Posts</h3>
    <div class="content">
        
          
          <span class="font-semibold">
            <i class="fas fa-th-list mr-1"></i>compiler-development »
          </span>
          <br />
          
            <span>1.</span>
            <a href="https://www.lei.chat/posts/compilers-and-irs-llvm-ir-spirv-and-mlir/">Compilers and IRs: LLVM IR, SPIR-V, and MLIR</a>
            <br />
          
            <span>2.</span>
            <a href="https://www.lei.chat/posts/mlir-codegen-dialects-for-machine-learning-compilers/">MLIR CodeGen Dialects for Machine Learning Compilers</a>
            <br />
          
            <span>3.</span>
            <a href="https://www.lei.chat/posts/mlir-vector-dialect-and-patterns/">MLIR Vector Dialect and Patterns</a>
            <br />
          
            <span>4.</span>
            <a href="https://www.lei.chat/posts/mlir-linalg-dialect-and-patterns/">MLIR Linalg Dialect and Patterns</a>
            <br />
          
            <span>5.</span>
            <a href="https://www.lei.chat/posts/single-node-ml-runtime-foundation/">Single-node ML Runtime Foundation</a>
            <br />
          
        
    </div>
</div>

        
        
        <div class="sticky top-16 z-10 hidden lg:block px-6 py-4  bg-primary-bg ">
    <span class="text-lg font-semibold">On This Page</span>
</div>
<div class="sticky-toc hidden lg:block px-6 pb-6 ">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#building-blocks">Building Blocks</a>
      <ul>
        <li><a href="#operations-carrying-structures">Operations carrying structures</a></li>
        <li><a href="#types-signaling-abstraction-levels">Types signaling abstraction levels</a></li>
        <li><a href="#dialects-as-modeling-granularity">Dialects as modeling granularity</a></li>
      </ul>
    </li>
    <li><a href="#dialect-hierarchy">Dialect Hierarchy</a>
      <ul>
        <li><a href="#problem-space">Problem space</a></li>
        <li><a href="#overall-picture">Overall picture</a></li>
        <li><a href="#high-level-model-dialects">High-level model dialects</a></li>
        <li><a href="#middle-level-lowering-dialects">Middle-level lowering dialects</a>
          <ul>
            <li><a href="#the-linalg-dialect">The linalg dialect</a></li>
            <li><a href="#tensors-tiling-and-fusion">Tensors, tiling and fusion</a></li>
            <li><a href="#buffers-distribution">Buffers, distribution</a></li>
            <li><a href="#the-tensor-memref-arith-math-dialect">The tensor, memref, arith, math dialect</a></li>
            <li><a href="#the-vector-dialect">The vector dialect</a></li>
            <li><a href="#vectorization-unrolling-hoisting-canonicalization">Vectorization, unrolling, hoisting, canonicalization</a></li>
            <li><a href="#the-scf-cf-dialect">The scf, cf dialect</a></li>
          </ul>
        </li>
        <li><a href="#low-level-target-dialects">Low-level target dialects</a></li>
      </ul>
    </li>
    <li><a href="#closing-words">Closing Words</a></li>
  </ul>
</nav>
</div>
<script>
    window.addEventListener('DOMContentLoaded', () => {
        enableStickyToc();
    });
</script>
        
    </div>
    

    
    
    <div
        class="col-span-2  lg:col-span-6 bg-secondary-bg rounded p-6">
        <h2 class="text-lg font-semibold mb-4">See Also</h2>
        <div class="content">
            
            <a href="https://www.lei.chat/posts/codegen-performant-convolution-kernels-for-mobile-gpus/">CodeGen Performant Convolution Kernels for Mobile GPUs</a>
            <br />
            
            <a href="https://www.lei.chat/posts/compilers-and-irs-llvm-ir-spirv-and-mlir/">Compilers and IRs: LLVM IR, SPIR-V, and MLIR</a>
            <br />
            
            <a href="https://www.lei.chat/zh/posts/compilers-and-irs-llvm-ir-spirv-and-mlir/">编译器与中间表示: LLVM IR, SPIR-V, 以及 MLIR</a>
            <br />
            
            <a href="https://www.lei.chat/posts/gpgpu-ml-inference-and-vulkan-compute/">GPGPU, ML Inference, and Vulkan Compute</a>
            <br />
            
            <a href="https://www.lei.chat/posts/shader-toolchain-hlsl-in-vulkan/">Shader Toolchain: HLSL in Vulkan</a>
            <br />
            
            <a href="https://www.lei.chat/posts/hlsl-for-vulkan-semantic-strings-and-location-numbers/">HLSL for Vulkan: Semantic Strings and Location Numbers</a>
            <br />
            
        </div>
    </div>
    
</div>
<script>
    document.addEventListener('DOMContentLoaded', ()=>{
        hljs.initHighlightingOnLoad();
    })
</script>

      </div>
    </div>
    
  </main>
  <footer class="pl-scrollbar">
    <div class="w-full max-w-screen-xl mx-auto"><div class="text-center p-6 pin-b">
    <p class="text-sm text-tertiary-text">&copy; 2018 - 2025 <a href="https://www.lei.chat/">Lei Zhang</a>
 &middot;  Powered by the <a href="https://github.com/wangchucheng/hugo-eureka" class="hover:text-eureka">Eureka</a> theme for <a href="https://gohugo.io" class="hover:text-eureka">Hugo</a></p>
</div></div>
  </footer>
</body>

</html>