<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ml on Lei.Chat()</title>
    <link>https://www.lei.chat/tags/ml/</link>
    <description>Recent content in ml on Lei.Chat()</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>&amp;copy; 2018 - 2023 &lt;a href=&#34;https://www.lei.chat/&#34;&gt;Lei Zhang&lt;/a&gt;
</copyright>
    <lastBuildDate>Sat, 01 Apr 2023 14:02:36 -0700</lastBuildDate><atom:link href="https://www.lei.chat/tags/ml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Single-node ML Runtime Foundation</title>
      <link>https://www.lei.chat/posts/single-node-ml-runtime-foundation/</link>
      <pubDate>Sat, 01 Apr 2023 14:02:36 -0700</pubDate>
      
      <guid>https://www.lei.chat/posts/single-node-ml-runtime-foundation/</guid>
      <description>&lt;p&gt;Previous blog posts overviewed the MLIR dialect hierarchy for &lt;a href=&#34;../mlir-codegen-dialects-for-machine-learning-compilers/&#34;&gt;kernel code
generation&lt;/a&gt; (CodeGen) and zoomed in on the
&lt;a href=&#34;../mlir-linalg-dialect-and-patterns/&#34;&gt;Linalg&lt;/a&gt; and &lt;a href=&#34;../mlir-vector-dialect-and-patterns/&#34;&gt;Vector&lt;/a&gt; dialects among them.
Now I will switch to discuss the runtime side a bit, in order to provide
a holistic view of MLIR-based machine learning (ML) compilers.
This one touches the foundation and basics, including the target landscape,
runtime requirements and designs to meet thereof.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MLIR CodeGen Dialects for Machine Learning Compilers</title>
      <link>https://www.lei.chat/posts/mlir-codegen-dialects-for-machine-learning-compilers/</link>
      <pubDate>Sun, 20 Feb 2022 15:21:03 -0500</pubDate>
      
      <guid>https://www.lei.chat/posts/mlir-codegen-dialects-for-machine-learning-compilers/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;../compilers-and-irs-llvm-ir-spirv-and-mlir/&#34;&gt;initial blog post&lt;/a&gt; in this series captured my overall take
on the evolution trends of compilers and IRs.
It also touched on &lt;a href=&#34;../compilers-and-irs-llvm-ir-spirv-and-mlir/#llvm-ir&#34;&gt;LLVM IR&lt;/a&gt;, &lt;a href=&#34;../compilers-and-irs-llvm-ir-spirv-and-mlir/#spir-v&#34;&gt;SPIR-V&lt;/a&gt;, and
&lt;a href=&#34;../compilers-and-irs-llvm-ir-spirv-and-mlir/#mlir&#34;&gt;MLIR&lt;/a&gt;, explaining the problems they are addressing and design
focuses thereof.
Today I will expand on MLIR and talk about its dialect hierarchy for machine
learning (ML) compilers systematically.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GPGPU, ML Inference, and Vulkan Compute</title>
      <link>https://www.lei.chat/posts/gpgpu-ml-inference-and-vulkan-compute/</link>
      <pubDate>Sun, 25 Jul 2021 11:25:26 -0400</pubDate>
      
      <guid>https://www.lei.chat/posts/gpgpu-ml-inference-and-vulkan-compute/</guid>
      <description>&lt;p&gt;Nowadays GPUs are utilized for both graphics rendering and general-purpose
compute (GPGPU). For the latter, CUDA is the indisputable leading solution.
Though, with so many other GPU vendors, the quest for a GPGPU standard never
stops. OpenCL was a great attempt and is used widely; but still it falls
short on many aspects.
Given the success of Vulkan in graphics and it being both a graphics and
compute API, one would wonder whether it can actually be the next-generation
GPGPU standard. I certainly believe so; but the road is not full of roses.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Edge/Mobile ML Inference Challenges</title>
      <link>https://www.lei.chat/posts/edge-mobile-ml-inference-challenges/</link>
      <pubDate>Sat, 17 Jul 2021 13:48:27 -0400</pubDate>
      
      <guid>https://www.lei.chat/posts/edge-mobile-ml-inference-challenges/</guid>
      <description>&lt;p&gt;These days if you would like to learn about machine learning, there are
abundant great resources on the web discussing model architectures and how to
code and train them.
Materials about inference, though, are generally much harder to find,
especially for edge and mobile. You might ask, inference is just the forward
pass of training, so how hard can it be? Actually, it faces lots of unique
challenges, to the extent that we are basically solving completely different
major problems.
I have been working on inference at the edge for a while, so let me capture
them in this blog post, by contrasting training and inference in the cloud.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
